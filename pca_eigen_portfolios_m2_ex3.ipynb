{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen-portfolio construction using Principal Component Analysis (PCA)\n",
    "\n",
    "### PCA via sklearn.decomposition using S&P 500 Index stock data\n",
    "\n",
    "Welcome to your 2-nd assignment in Unsupervised Machine Learning in Finance.\n",
    "\n",
    "In this assignment we look in-depth at model-free factor analysis using PCA. By model-free we mean that we do not rely on any factors such as value or momentum to decompose portfolio returns, but instead using Principal Component Analysis (PCA) to deduce structure of portfolio returns.\n",
    "\n",
    "We work with S&P 500 index stock data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. You only need to write code between the ### START CODE HERE ### and ### END CODE HERE ### comments. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run Cell\" (denoted by a play symbol) in the upper bar of the notebook. \n",
    "\n",
    "We will often specify \"(≈ X lines of code)\" in the comments to tell you about how much code you need to write. It is just a rough estimate, so don't feel bad if your code is longer or shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import grading\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"  pandas: %s\"% pd.__version__)\n",
    "except:\n",
    "    print(\"Missing pandas package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ONLY FOR GRADING. DO NOT EDIT ### \n",
    "submissions=dict()\n",
    "assignment_key=\"BBz-XobeEeegARIApDSa9g\" \n",
    "all_parts=[\"nvDA9\", \"ykDlW\", \"rpYVm\",\"oWy6l\",\"MWWt7\",\"3VyJD\"]\n",
    "### ONLY FOR GRADING. DO NOT EDIT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COURSERA_TOKEN = \" \"  # the key provided to the Student under his/her email on submission page\n",
    "COURSERA_EMAIL =  \" \"  # the email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "asset_prices = pd.read_csv('/home/jovyan/work/readonly/spx_holdings_and_spx_closeprice.csv',\n",
    "                     date_parser=lambda dt: pd.to_datetime(dt, format='%Y-%m-%d'),\n",
    "                     index_col = 0).dropna()\n",
    "n_stocks_show = 12\n",
    "print('Asset prices shape', asset_prices.shape)\n",
    "asset_prices.iloc[:, :n_stocks_show].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Last column contains SPX index prices:')\n",
    "asset_prices.iloc[:, -10:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 (Asset Returns Calculation)\n",
    "**Instructions:**\n",
    "\n",
    "Calculate percent returns, also known as simple returns using asse_prices. assign the result to variable asset_returns. Keep only not-nan values in the resulting pandas.DataFrame\n",
    "\n",
    "Calculate de-meaned returns and scale them by standard deviation $\\sigma$. Assign result to normed_returns variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute stock returns and normalize stock returns data by subtracting the mean and dividing by standard diviation. This normalization is required by PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_returns = pd.DataFrame(data=np.zeros(shape=(len(asset_prices.index), asset_prices.shape[1])), \n",
    "                             columns=asset_prices.columns.values,\n",
    "                             index=asset_prices.index)\n",
    "normed_returns = asset_returns\n",
    "### START CODE HERE ### (≈ 4 lines of code)\n",
    "# normed_returns is pandas.DataFrame that should contain normalized returns\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "normed_returns.iloc[-5:, -10:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_1=list(normed_returns.iloc[0,: 100].as_matrix().squeeze())\n",
    "try:\n",
    "    part1 = \" \".join(map(repr, part_1))\n",
    "except TypeError:\n",
    "    part1 = repr(part_1)\n",
    "submissions[all_parts[0]]=part1\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:1],all_parts,submissions)\n",
    "normed_returns.iloc[0,: 100].as_matrix().squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = datetime.datetime(2012, 3, 26) \n",
    "\n",
    "df_train = None\n",
    "df_test = None\n",
    "df_raw_train = None\n",
    "df_raw_test = None\n",
    "\n",
    "df_train = normed_returns[normed_returns.index <= train_end].copy()\n",
    "df_test = normed_returns[normed_returns.index > train_end].copy()\n",
    "\n",
    "df_raw_train = asset_returns[asset_returns.index <= train_end].copy()\n",
    "df_raw_test = asset_returns[asset_returns.index > train_end].copy()\n",
    "\n",
    "print('Train dataset:', df_train.shape)\n",
    "print('Test dataset:', df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute PCA using all available data. Once we do have PCA computed we fix variance explained at some number and see what is the smallest number of components needed to explain this variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 (PCA fitting)\n",
    "**Instructions:**\n",
    "- Calculate covariance matrix using training data set, i.e. **df_train** for all assets.  Assign results to **cov_matrix**.\n",
    "- Calculate covariance matrix using training data set, i.e. **df_raw_train** for all assets.  Assign results to **cov_matrix_raw**.\n",
    "- Use scikit-learn PCA to fit PCA model to **cov_matrix**. Assign fitted model to **pca**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "import seaborn as sns\n",
    "\n",
    "stock_tickers = normed_returns.columns.values[:-1]\n",
    "assert 'SPX' not in stock_tickers, \"By accident included SPX index\"\n",
    "\n",
    "n_tickers = len(stock_tickers)\n",
    "pca = None\n",
    "cov_matrix = pd.DataFrame(data=np.ones(shape=(n_tickers, n_tickers)), columns=stock_tickers)\n",
    "cov_matrix_raw = cov_matrix\n",
    "\n",
    "if df_train is not None and df_raw_train is not None:\n",
    "    stock_tickers = asset_returns.columns.values[:-1]\n",
    "    assert 'SPX' not in stock_tickers, \"By accident included SPX index\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2-3 lines of code)\n",
    "    \n",
    "    # computing PCA on S&P 500 stocks\n",
    "    \n",
    "\n",
    "    # not normed covariance matrix\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cov_raw_df = pd.DataFrame({'Variance': np.diag(cov_matrix_raw)}, index=stock_tickers)    \n",
    "    # cumulative variance explained\n",
    "    var_threshold = 0.8\n",
    "    var_explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "    num_comp = np.where(np.logical_not(var_explained < var_threshold))[0][0] + 1  # +1 due to zero based-arrays\n",
    "    print('%d components explain %.2f%% of variance' %(num_comp, 100* var_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_2 = np.diag(cov_matrix[: 100])\n",
    "try:\n",
    "    part2 = \" \".join(map(repr, part_2))\n",
    "except TypeError:\n",
    "    part2 = repr(part_2)\n",
    "submissions[all_parts[1]]=part2\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:2],all_parts,submissions)\n",
    "### GRADED PART (DO NOT EDIT) ###\n",
    "np.diag(cov_matrix[: 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if pca is not None:\n",
    "    bar_width = 0.9\n",
    "    n_asset = int((1 / 10) * normed_returns.shape[1])\n",
    "    x_indx = np.arange(n_asset)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(12, 4)\n",
    "    # Eigenvalues are measured as percentage of explained variance.\n",
    "    rects = ax.bar(x_indx, pca.explained_variance_ratio_[:n_asset], bar_width, color='deepskyblue')\n",
    "    ax.set_xticks(x_indx + bar_width / 2)\n",
    "    ax.set_xticklabels(list(range(n_asset)), rotation=45)\n",
    "    ax.set_title('Percent variance explained')\n",
    "    ax.legend((rects[0],), ('Percent variance explained by principal components',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if pca is not None:\n",
    "    projected = pca.fit_transform(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 (Eigen-portfolios construction)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "We now look a the first two eigen portfolios. We use definition of eigen portfolios as provided by Avellaneda \n",
    "http://math.nyu.edu/faculty/avellane/AvellanedaLeeStatArb20090616.pdf\n",
    "\n",
    "Following Avellaneda we define eigen portfolio weights as:\n",
    "$$Q_i^{(j)} = \\frac{v_i^{(j)}}{\\sigma_i}$$\n",
    "\n",
    "where $j$ is the index of eigen portfolio and $v_i$ is the i-th element of j-th eigen vector.\n",
    "\n",
    "In the code the pca.components_ are the Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_.\n",
    "\n",
    "**Hint:** do not forget to normalize portfolio wieghts such they sum up to 1.\n",
    "\n",
    "Assign **pc_w** to be weights of the first eigen portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the first two eigen-portfolio weights# the fi \n",
    "# first component\n",
    "# get the Principal components\n",
    "pc_w = np.zeros(len(stock_tickers))\n",
    "eigen_prtf1 = pd.DataFrame(data ={'weights': pc_w.squeeze()*100}, index = stock_tickers)\n",
    "if pca is not None:\n",
    "    pcs = pca.components_\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1-2 lines of code)\n",
    "    # normalized to 1 \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    eigen_prtf1 = pd.DataFrame(data ={'weights': pc_w.squeeze()*100}, index = stock_tickers)\n",
    "    eigen_prtf1.sort_values(by=['weights'], ascending=False, inplace=True)\n",
    "    print('Sum of weights of first eigen-portfolio: %.2f' % np.sum(eigen_prtf1))\n",
    "    eigen_prtf1.plot(title='First eigen-portfolio weights', \n",
    "                     figsize=(12,6), \n",
    "                     xticks=range(0, len(stock_tickers),10), \n",
    "                     rot=45, \n",
    "                     linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_3 = list(eigen_prtf1.squeeze().values)\n",
    "try:\n",
    "    part3 = \" \".join(map(repr, part_3))\n",
    "except TypeError:\n",
    "    part3 = repr(part_3)\n",
    "submissions[all_parts[2]]=part3\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:3],all_parts,submissions)\n",
    "eigen_prtf1.squeeze().values\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the first two eigen portfolio weights and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_w = np.zeros(len(stock_tickers))\n",
    "eigen_prtf2 = pd.DataFrame(data ={'weights': pc_w.squeeze()*100}, index = stock_tickers)\n",
    "\n",
    "if pca is not None:\n",
    "    pcs = pca.components_\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1-2 lines of code)\n",
    "    # normalized to 1 \n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    eigen_prtf2 = pd.DataFrame(data ={'weights': pc_w.squeeze()*100}, index = stock_tickers)\n",
    "    eigen_prtf2.sort_values(by=['weights'], ascending=False, inplace=True)\n",
    "    print('Sum of weights of second eigen-portfolio: %.2f' % np.sum(eigen_prtf2))\n",
    "    eigen_prtf2.plot(title='Second eigen-portfolio weights',\n",
    "                     figsize=(12,6), \n",
    "                     xticks=range(0, len(stock_tickers),10), \n",
    "                     rot=45, \n",
    "                     linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_4 = list(eigen_prtf2.as_matrix().squeeze())\n",
    "try:\n",
    "    part4 = \" \".join(map(repr, part_4))\n",
    "except TypeError:\n",
    "    part4 = repr(part_4)\n",
    "submissions[all_parts[3]]=part4\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:4],all_parts,submissions)\n",
    "eigen_prtf2.as_matrix().squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 (Compute performance of several eigen portfolios)\n",
    "\n",
    "**Instructions:**\n",
    "- Implement sharpe_ratio() function. The function takes ts_returns argument of type pd.Series and returns a tuple of annualized return, annualized vol, and annualized sharpe ratio, where sharpe ratio is defined as annualized return divided by annualized volatility \n",
    "- find portfolio (an index into sharpe_metric) that has the highest sharpe ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sharpe_ratio(ts_returns, periods_per_year=252):\n",
    "    \"\"\"\n",
    "    sharpe_ratio - Calculates annualized return, annualized vol, and annualized sharpe ratio, \n",
    "                    where sharpe ratio is defined as annualized return divided by annualized volatility \n",
    "                    \n",
    "    Arguments:\n",
    "    ts_returns - pd.Series of returns of a single eigen portfolio\n",
    "    \n",
    "    Return:\n",
    "    a tuple of three doubles: annualized return, volatility, and sharpe ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    annualized_return = 0.\n",
    "    annualized_vol = 0.\n",
    "    annualized_sharpe = 0.\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4-5 lines of code)\n",
    "    ### ...\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return annualized_return, annualized_vol, annualized_sharpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the annualized return, volatility, and Sharpe ratio of the first two eigen portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_raw_test is not None:\n",
    "    eigen_prtf1_returns = np.dot(df_raw_test.loc[:, eigen_prtf1.index], eigen_prtf1 / 100)\n",
    "    eigen_prtf1_returns = pd.Series(eigen_prtf1_returns.squeeze(), index=df_test.index)\n",
    "    er, vol, sharpe = sharpe_ratio(eigen_prtf1_returns)\n",
    "    print('First eigen-portfolio:\\nReturn = %.2f%%\\nVolatility = %.2f%%\\nSharpe = %.2f' % (er*100, vol*100, sharpe))\n",
    "    year_frac = (eigen_prtf1_returns.index[-1] - eigen_prtf1_returns.index[0]).days / 252\n",
    "\n",
    "    df_plot = pd.DataFrame({'PC1': eigen_prtf1_returns, 'SPX': df_raw_test.loc[:, 'SPX']}, index=df_test.index)\n",
    "    np.cumprod(df_plot + 1).plot(title='Returns of the market-cap weighted index vs. First eigen-portfolio', \n",
    "                             figsize=(12,6), linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_raw_test is not None:\n",
    "    eigen_prtf2_returns = np.dot(df_raw_test.loc[:, eigen_prtf2.index], eigen_prtf2 / 100)\n",
    "    eigen_prtf2_returns = pd.Series(eigen_prtf2_returns.squeeze(), index=df_test.index)\n",
    "    er, vol, sharpe = sharpe_ratio(eigen_prtf2_returns)\n",
    "    print('Second eigen-portfolio:\\nReturn = %.2f%%\\nVolatility = %.2f%%\\nSharpe = %.2f' % (er*100, vol*100, sharpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the exercise of computing Sharpe ratio for the first N portfolios and select portfolio with the highest postive Sharpe ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_portfolios = 120\n",
    "annualized_ret = np.array([0.] * n_portfolios)\n",
    "sharpe_metric = np.array([0.] * n_portfolios)\n",
    "annualized_vol = np.array([0.] * n_portfolios)\n",
    "idx_highest_sharpe = 0 # index into sharpe_metric which identifies a portfolio with rhe highest Sharpe ratio\n",
    "    \n",
    "if pca is not None:\n",
    "    for ix in range(n_portfolios):\n",
    "        \n",
    "        ### START CODE HERE ### (≈ 4-5 lines of code)\n",
    "        \n",
    "    \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    # find portfolio with the highest Sharpe ratio\n",
    "    ### START CODE HERE ### (≈ 2-3 lines of code)\n",
    "    ### ...\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    print('Eigen portfolio #%d with the highest Sharpe. Return %.2f%%, vol = %.2f%%, Sharpe = %.2f' % \n",
    "          (idx_highest_sharpe,\n",
    "           annualized_ret[idx_highest_sharpe]*100, \n",
    "           annualized_vol[idx_highest_sharpe]*100, \n",
    "           sharpe_metric[idx_highest_sharpe]))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(12, 4)\n",
    "    ax.plot(sharpe_metric, linewidth=3)\n",
    "    ax.set_title('Sharpe ratio of eigen-portfolios')\n",
    "    ax.set_ylabel('Sharpe ratio')\n",
    "    ax.set_xlabel('Portfolios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data={'Return': annualized_ret, 'Vol': annualized_vol, 'Sharpe': sharpe_metric})\n",
    "results.sort_values(by=['Sharpe'], ascending=False, inplace=True)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_5 = list(results.iloc[:, 1].values.squeeze())\n",
    "try:\n",
    "    part5 = \" \".join(map(repr, part_5))\n",
    "except TypeError:\n",
    "    part5 = repr(part_5)\n",
    "submissions[all_parts[4]]=part5\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:5],all_parts,submissions)\n",
    "results.iloc[:, 1].values.squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part6 = str(idx_highest_sharpe)\n",
    "submissions[all_parts[5]]=part6\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:6],all_parts,submissions)\n",
    "idx_highest_sharpe\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "coursera": {
   "course_slug": "machine-learning-in-finance"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
